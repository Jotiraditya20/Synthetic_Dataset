{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "# === CONFIG ===\n",
    "image_path = r\"C:\\Users\\jotir\\Downloads\\train-2\\images\\3.jpg\"  # Change to your image\n",
    "label_path = r\"C:\\Users\\jotir\\Downloads\\train-2\\labels\\3.txt\"  # Ground truth label\n",
    "model_path = \"runs/detect/train11/weights/last.pt\"  # YOLOv12 model\n",
    "output_dir = 'runs/detect/eval'  # YOLOv12 output folder\n",
    "iou_thresholds = np.arange(0.5, 1.0, 0.05)  # IoU thresholds for mAP50-95\n",
    "class_names = ['image', 'text']  # class 0 = image, 1 = text\n",
    "\n",
    "# === STEP 1: Load the YOLOv12 model ===\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# === STEP 2: Run YOLOv12 Prediction ===\n",
    "results = model(image_path, conf=0.8)  # Run inference with the given image and confidence threshold\n",
    "\n",
    "# === STEP 3: Load Ground Truth YOLO Label ===\n",
    "h, w, _ = cv2.imread(image_path).shape\n",
    "gt_boxes = []\n",
    "with open(label_path, 'r') as f:\n",
    "    for line in f:\n",
    "        cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
    "        x1 = (cx - bw/2) * w\n",
    "        y1 = (cy - bh/2) * h\n",
    "        x2 = (cx + bw/2) * w\n",
    "        y2 = (cy + bh/2) * h\n",
    "        gt_boxes.append([x1, y1, x2, y2, int(cls)])\n",
    "gt_boxes = np.array(gt_boxes)\n",
    "\n",
    "# === STEP 4: IoU Calculation ===\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "# === STEP 5: Extract Prediction Boxes ===\n",
    "# Get prediction boxes and labels\n",
    "pred_boxes = []\n",
    "for det in results:\n",
    "    boxes = det.boxes.xyxy.cpu().numpy()  # Get the xyxy coordinates of predicted boxes\n",
    "    confs = det.boxes.conf.cpu().numpy()  # Get the confidence scores\n",
    "    clses = det.boxes.cls.cpu().numpy()  # Get the class ids\n",
    "    for box, conf, cls in zip(boxes, confs, clses):\n",
    "        x1, y1, x2, y2 = box\n",
    "        pred_boxes.append([x1, y1, x2, y2, conf, int(cls)])\n",
    "pred_boxes = np.array(pred_boxes)\n",
    "\n",
    "# === STEP 6: Evaluate ===\n",
    "TP = {cls: 0 for cls in range(len(class_names))}\n",
    "FP = {cls: 0 for cls in range(len(class_names))}\n",
    "FN = {cls: 0 for cls in range(len(class_names))}\n",
    "IoUs = defaultdict(list)  # To store IoUs for each prediction\n",
    "confidences = defaultdict(list)  # Store confidence for each class\n",
    "matched = set()\n",
    "\n",
    "# Calculate per-class TP, FP, FN and IoU\n",
    "for pred in pred_boxes:\n",
    "    matched_flag = False\n",
    "    for i, gt in enumerate(gt_boxes):\n",
    "        if i in matched: continue\n",
    "        if pred[5] == gt[4] and iou(pred[:4], gt[:4]) >= iou_thresholds[0]:\n",
    "            TP[pred[5]] += 1\n",
    "            IoUs[pred[5]].append(iou(pred[:4], gt[:4]))  # Store the IoU for this class\n",
    "            matched.add(i)\n",
    "            matched_flag = True\n",
    "            break\n",
    "    if not matched_flag:\n",
    "        FP[pred[5]] += 1\n",
    "\n",
    "# Calculate FN for each class\n",
    "for i, gt in enumerate(gt_boxes):\n",
    "    if i not in matched:\n",
    "        FN[gt[4]] += 1\n",
    "\n",
    "# Calculate precision, recall, and IoU for each class and overall\n",
    "precision = {cls: TP[cls] / (TP[cls] + FP[cls]) if (TP[cls] + FP[cls]) > 0 else 0 for cls in range(len(class_names))}\n",
    "recall = {cls: TP[cls] / (TP[cls] + FN[cls]) if (TP[cls] + FN[cls]) > 0 else 0 for cls in range(len(class_names))}\n",
    "IoU = {cls: np.mean(IoUs[cls]) if IoUs[cls] else 0 for cls in range(len(class_names))}\n",
    "\n",
    "# Overall precision and recall\n",
    "total_TP = sum(TP.values())\n",
    "total_FP = sum(FP.values())\n",
    "total_FN = sum(FN.values())\n",
    "\n",
    "overall_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "overall_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "\n",
    "# === STEP 7: Calculate mAP@0.5 and mAP@0.5:0.95 ===\n",
    "def calc_map(pred_boxes, gt_boxes, iou_thresholds):\n",
    "    ap_per_class = {}\n",
    "    for cls in range(len(class_names)):\n",
    "        # Placeholder for calculating AP (using IoU thresholds)\n",
    "        ap_per_class[cls] = {'mAP50': 0, 'mAP50-95': 0}\n",
    "        # In practice, you would implement AP calculation using the precision-recall curve at different IoU thresholds.\n",
    "    return ap_per_class\n",
    "\n",
    "ap_per_class = calc_map(pred_boxes, gt_boxes, iou_thresholds)\n",
    "\n",
    "# === PRINT EVALUATION RESULTS ===\n",
    "print(\"\\n=== ðŸ“Š YOLOv12 Evaluation (Single Image) ===\")\n",
    "print(f\"Overall Precision: {overall_precision:.3f}\")\n",
    "print(f\"Overall Recall: {overall_recall:.3f}\")\n",
    "print(f\"Overall IoU: {np.mean([IoU[cls] for cls in IoU]):.3f}\")\n",
    "\n",
    "# Print FP, TP, FN for each class along with precision, recall, IoU\n",
    "for cls, name in enumerate(class_names):\n",
    "    print(f\"\\nClass: {name}\")\n",
    "    print(f\"  True Positives (TP): {TP[cls]}\")\n",
    "    print(f\"  False Positives (FP): {FP[cls]}\")\n",
    "    print(f\"  False Negatives (FN): {FN[cls]}\")\n",
    "    print(f\"  Precision: {precision[cls]:.3f}\")\n",
    "    print(f\"  Recall: {recall[cls]:.3f}\")\n",
    "    print(f\"  IoU: {IoU[cls]:.3f}\")\n",
    "    print(f\"  mAP50: {ap_per_class[cls]['mAP50']:.3f}\")\n",
    "    print(f\"  mAP50-95: {ap_per_class[cls]['mAP50-95']:.3f}\")\n",
    "\n",
    "# === STEP 8: Visualize Results ===\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Visualize Ground Truth Boxes (Green) and Predicted Boxes (Blue)\n",
    "for x1, y1, x2, y2, cls in gt_boxes:\n",
    "    cls = int(cls)  # Ensure that cls is an integer\n",
    "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "    cv2.putText(img, f\"GT {class_names[cls]}\", (int(x1), int(y1)-5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "for x1, y1, x2, y2, conf, cls in pred_boxes:\n",
    "    cls = int(cls)  # Ensure that cls is an integer\n",
    "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "    cv2.putText(img, f\"Pred {class_names[cls]} {conf:.2f}\", (int(x1), int(y1)-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "# Show the image with bounding boxes\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Predictions (Blue) vs GT (Green)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e47a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
